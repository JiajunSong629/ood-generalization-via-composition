{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model_name):\n",
    "    dir = os.path.join(\"out\", model_name)\n",
    "\n",
    "    diagonal = json.load(open(os.path.join(dir, \"diagonal.json\"), \"r\"))\n",
    "    subspace = json.load(\n",
    "        open(os.path.join(dir, \"subspace_IH_PTH_K40_largest.json\"), \"r\")\n",
    "    )\n",
    "\n",
    "    return diagonal, subspace\n",
    "\n",
    "\n",
    "def select(model_name, score_cutoff=2, head_count_max=10):\n",
    "    diagonal, subspace = load(model_name)\n",
    "\n",
    "    # select by diagonal\n",
    "    ih, pth = [], []\n",
    "    # for d in diagonal:\n",
    "    #     if d[\"score\"] > score_cutoff:\n",
    "    #         ih.append(tuple(d[\"IH\"]))\n",
    "    #         pth.append(tuple(d[\"PTH\"]))\n",
    "    idx = 0\n",
    "    while (len(ih) < head_count_max or len(pth) < head_count_max) and idx < len(\n",
    "        diagonal\n",
    "    ):\n",
    "        d = diagonal[idx]\n",
    "        if d[\"score\"] < score_cutoff:\n",
    "            idx += 1\n",
    "            continue\n",
    "        if len(ih) < head_count_max and d[\"IH\"] not in ih:\n",
    "            ih.append(d[\"IH\"])\n",
    "\n",
    "        if len(pth) < head_count_max and d[\"PTH\"] not in pth:\n",
    "            pth.append(d[\"PTH\"])\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    # ih = list(set(ih))[:head_count_max]\n",
    "    # pth = list(set(pth))[:head_count_max]\n",
    "\n",
    "    print(model_name)\n",
    "    print(f\"DIAGONAL\\nIH: {ih}\\nPTH: {pth}\\n\" + \"-\" * 50)\n",
    "    save_dir = f\"checkpoints/{model_name}\"\n",
    "    torch.save(ih, f\"{save_dir}/IH_subset_diagonal.pt\")\n",
    "    torch.save(pth, f\"{save_dir}/PTH_subset_diagonal.pt\")\n",
    "\n",
    "    # select by subspace\n",
    "    ih, pth = [], []\n",
    "    idx = 1\n",
    "    while len(ih) < head_count_max or len(pth) < head_count_max:\n",
    "        s = subspace[idx]\n",
    "        if len(ih) < head_count_max and s[\"LH0\"] not in ih:\n",
    "            ih.append(s[\"LH0\"])\n",
    "\n",
    "        if len(pth) < head_count_max and s[\"LH1\"] not in pth:\n",
    "            pth.append(s[\"LH1\"])\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    print(f\"SUBSPACE\\nIH: {ih}\\nPTH: {pth}\\n\" + \"-\" * 50)\n",
    "    save_dir = f\"checkpoints/{model_name}\"\n",
    "    torch.save(ih, f\"{save_dir}/IH_subset_subspace.pt\")\n",
    "    torch.save(pth, f\"{save_dir}/PTH_subset_subspace.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2\n",
      "DIAGONAL\n",
      "IH: [[5, 1], [6, 9], [5, 5], [7, 2], [7, 10], [5, 8], [5, 0], [8, 1], [7, 11], [9, 6]]\n",
      "PTH: [[4, 11], [5, 6], [8, 7], [6, 8], [6, 0], [9, 3], [3, 3], [7, 0], [5, 2], [1, 0]]\n",
      "--------------------------------------------------\n",
      "SUBSPACE\n",
      "IH: [[5, 1], [5, 0], [7, 11], [7, 1], [6, 9], [8, 1], [5, 8], [7, 10], [7, 2], [7, 7]]\n",
      "PTH: [[4, 11], [6, 0], [6, 8], [5, 6], [4, 6], [4, 3], [6, 5], [5, 2], [7, 0], [10, 9]]\n",
      "--------------------------------------------------\n",
      "gpt2-xl\n",
      "DIAGONAL\n",
      "IH: [[17, 6], [16, 21], [16, 3], [13, 0], [18, 0], [17, 14], [20, 0], [19, 18], [22, 20], [21, 3]]\n",
      "PTH: [[15, 19], [12, 21], [13, 20], [14, 12], [16, 5], [11, 2], [9, 7], [14, 20], [10, 15], [13, 12]]\n",
      "--------------------------------------------------\n",
      "SUBSPACE\n",
      "IH: [[17, 6], [16, 21], [13, 0], [17, 1], [21, 3], [20, 0], [25, 18], [21, 20], [16, 3], [28, 11]]\n",
      "PTH: [[15, 19], [13, 20], [16, 5], [12, 21], [14, 12], [14, 20], [17, 12], [6, 1], [9, 7], [11, 2]]\n",
      "--------------------------------------------------\n",
      "llama2-7b\n",
      "DIAGONAL\n",
      "IH: [[6, 9], [6, 30], [7, 4], [8, 26], [7, 12], [7, 13], [6, 11], [8, 31], [6, 16], [11, 15]]\n",
      "PTH: [[5, 15], [6, 5], [10, 3], [15, 11], [10, 25], [19, 2], [9, 27], [10, 17], [4, 0], [4, 3]]\n",
      "--------------------------------------------------\n",
      "SUBSPACE\n",
      "IH: [[7, 4], [8, 26], [7, 12], [6, 30], [8, 31], [7, 13], [6, 9], [7, 10], [6, 16], [11, 15]]\n",
      "PTH: [[6, 5], [5, 15], [10, 25], [10, 3], [5, 16], [1, 22], [15, 11], [2, 30], [7, 9], [3, 19]]\n",
      "--------------------------------------------------\n",
      "gemma-7b\n",
      "DIAGONAL\n",
      "IH: [[5, 0]]\n",
      "PTH: [[3, 12], [1, 0], [13, 3]]\n",
      "--------------------------------------------------\n",
      "SUBSPACE\n",
      "IH: [[5, 0], [24, 3], [20, 1], [21, 3], [16, 1], [27, 14], [21, 2], [22, 7], [21, 1], [21, 5]]\n",
      "PTH: [[3, 12], [27, 5], [1, 0], [17, 0], [11, 3], [13, 3], [10, 14], [23, 15], [11, 15], [10, 10]]\n",
      "--------------------------------------------------\n",
      "falcon-7b\n",
      "DIAGONAL\n",
      "IH: [[5, 65], [5, 18], [5, 13], [5, 10], [5, 2], [5, 1], [5, 69], [5, 43], [5, 41], [5, 14]]\n",
      "PTH: [[3, 38], [2, 40], [2, 16], [1, 34], [2, 21], [17, 53], [1, 29], [17, 24], [3, 56], [1, 9]]\n",
      "--------------------------------------------------\n",
      "SUBSPACE\n",
      "IH: [[5, 41], [5, 59], [5, 2], [5, 5], [5, 46], [5, 70], [5, 52], [5, 33], [5, 21], [5, 66]]\n",
      "PTH: [[3, 38], [17, 53], [17, 24], [2, 40], [4, 30], [2, 21], [1, 34], [23, 5], [4, 16], [23, 46]]\n",
      "--------------------------------------------------\n",
      "mistral-7b\n",
      "DIAGONAL\n",
      "IH: [[12, 6], [12, 4], [12, 7], [18, 2], [18, 1], [18, 3], [18, 0], [2, 22], [2, 21], [11, 14]]\n",
      "PTH: [[11, 17], [17, 22], [1, 0], [10, 14], [24, 8], [0, 29], [29, 4], [10, 12], [29, 6]]\n",
      "--------------------------------------------------\n",
      "SUBSPACE\n",
      "IH: [[12, 6], [12, 7], [12, 4], [18, 1], [18, 2], [18, 0], [18, 3], [11, 14], [20, 30], [20, 29]]\n",
      "PTH: [[11, 17], [17, 22], [10, 14], [24, 8], [6, 24], [29, 4], [29, 6], [3, 12], [0, 10], [1, 0]]\n",
      "--------------------------------------------------\n",
      "olmo-7b\n",
      "DIAGONAL\n",
      "IH: [[27, 14], [15, 15], [24, 7], [2, 28], [2, 10], [26, 17], [2, 16], [27, 15], [27, 26], [2, 22]]\n",
      "PTH: [[26, 25], [14, 30], [14, 18], [23, 24], [1, 7], [24, 3], [25, 6], [1, 15], [26, 30], [28, 3]]\n",
      "--------------------------------------------------\n",
      "SUBSPACE\n",
      "IH: [[27, 14], [26, 17], [15, 15], [2, 10], [2, 28], [2, 22], [24, 7], [30, 13], [29, 12], [29, 29]]\n",
      "PTH: [[26, 30], [24, 3], [26, 25], [14, 18], [1, 7], [23, 24], [14, 30], [25, 6], [28, 3], [19, 3]]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for model_name in [\n",
    "    \"gpt2\",\n",
    "    \"gpt2-xl\",\n",
    "    \"llama2-7b\",\n",
    "    \"gemma-7b\",\n",
    "    \"falcon-7b\",\n",
    "    \"mistral-7b\",\n",
    "    \"olmo-7b\",\n",
    "]:\n",
    "    select(model_name, score_cutoff=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
